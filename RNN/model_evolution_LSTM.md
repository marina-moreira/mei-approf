### Training Model Comparison

Here are the differences between the models based on their architectures, hyperparameters, performance metrics, and regularization functions applied.

| Model                                 | Architecture                                                                                                                                                                                                                                      | Time Window | Epochs | Batch Size | Learning Rate | Optimizer | Loss Function            | Training Accuracy | Test Accuracy | Training Loss | Test Loss | Regularization Functions                                        | Key Changes                                                                                                  |
|---------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|--------|------------|---------------|-----------|--------------------------|-------------------|---------------|---------------|-----------|-----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| **custom_rnn_lstm_singleLayer_model_7_19** | **LSTM(50, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.96 (96%)        | 0.97 (97%)    | 0.087         | 0.09     | Learning Rate Scheduler<br> Early Stopping                         | Initial model with data augmentation                                                                       |
| custom_rnn_lstm_singleLayer_model_7_20       | LSTM(50, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)    | 0.067         | 0.068     | Learning Rate Scheduler<br> Early Stopping                         | Changing the time window                    |
| custom_rnn_lstm_singleLayer_model_7_21       | LSTM(50, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)    | 0.049         | 0.049     | Learning Rate Scheduler<br> Early Stopping                         | Changing the time window                                           |
| **custom_rnn_lstm_singleLayer_model_8_22**       | **LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES)), Dropout(0.5), Dense(100, activation='relu'), Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.99 (99%)        |  0.99 (99%)    | 0.000821         | 0.200108     | Dropout<br>Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense and LSTM layers                    |
| custom_rnn_lstm_singleLayer_model_8_23       | LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES)), Dropout(0.5), Dense(100, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)    | 0.063         | 0.063     | Dropout<br>Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense and LSTM  layers and Changing the time window                                   |
| custom_rnn_lstm_singleLayer_model_8_24       |LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES)), Dropout(0.5), Dense(100, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)    | 0.054         | 0.053     | Dropout<br>Learning Rate Scheduler<br> Early Stopping <br> | Changing units on the Dense and LSTM and Changing the time window layers                                              |
| **custom_rnn_lstm_singleLayer_model_9_25**       | **LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dropout(0.5), BatchNormalization(), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.0.99 (99%)        |  0.99 (99%)    | 0.0092         | 0.00852    | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense layer, Adding Batch Normalization and Changing the time window        |
| custom_rnn_lstm_singleLayer_model_9_26       |LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dropout(0.5), BatchNormalization(), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.0.99 (99%)        |  0.99 (99%)    | 0.000462         | 0.000681     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense layer, Adding Batch Normalization and Changing the time window                  |
| custom_rnn_lstm_singleLayer_model_9_27       | LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dropout(0.5), BatchNormalization(), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 32         | 0.0010        | Adam      | Categorical Crossentropy | 0.0.98 (98%)        | 0.98 (98%)    | 0.050         | 0.0489     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense layer, Adding Batch Normalization and Changing the time window
| **custom_rnn_lstm_multiLayer_model_10_28**       | **LSTM(units=30, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.2), LSTM(units=30, return_sequences=False), Dropout(0.2), Dense(units=15, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.97 (97%)        | 0.97 (97%)          | 0.0793          | 0.0809            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 30 units, adding onde more dropout(0.2) and Changing units values |
| custom_rnn_lstm_multiLayer_model_10_29    | LSTM(units=30, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.2), LSTM(units=30, return_sequences=False), Dropout(0.2), Dense(units=15, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax') | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.99 (99%)        | 0.99 (99%)          | 0.0012          | 0.000786            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 30 units, adding onde more dropout(0.2) and Changing units values |
| custom_rnn_lstm_multiLayer_model_10_30   | LSTM(units=30, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.2), LSTM(units=30, return_sequences=False), Dropout(0.2), Dense(units=15, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax') | 50          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.99 (99%)        | 0.99 (99%)          | 0.00000793          | 0.0000129            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 30 units, adding onde more dropout(0.2) and Changing units values
| **custom_rnn_lstm_multiLayer_model_11_31**   | **LSTM(units=64, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=False), Dropout(0.3), Dense(units=64, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax')** | 20          | 30     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.97 (97%)        | 0.97 (97%)          | 0.0875          | 0.0.897            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 64 units, Changing LSTM layer units to 64, Changing dropout values to 0.3    
| custom_rnn_lstm_multiLayer_model_11_32   | LSTM(units=64, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=False), Dropout(0.3), Dense(units=64, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax') | 30          | 30     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)          | 0.0607          | 0.0612            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 64 units, Changing LSTM layer units to 64, Changing dropout values to 0.3      
| custom_rnn_lstm_multiLayer_model_11_33   | LSTM(units=64, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=False), Dropout(0.3), Dense(units=64, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax') | 50          | 30     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.99 (99%)        | 0.99 (99%)          | 0.0435          | 0.0428            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 64 units, Changing LSTM layer units to 64, Changing dropout values to 0.3  |                                                   |

## Analysis

1. **Architecture**:
    - The architecture changes in order to be improved and each one is trained with 3 different Time Window values.

2. **Hyperparameters**:
    - The epochs, batch size, optimizer, and loss function remain constant across iterations to isolate the effect of architecture changes on performance.

3. **Regularization**:
    - Dropout layers are added to help prevent overfitting.

4. **Performance**:
    - The performance of the models varies according to the variation in time window values ​​as well as the architecture, with no progressive improvement.

This table provides a comprehensive summary of the key changes and performance metrics for each model iteration.

## Analysis

The best and worst case scenarios are:
  - **Best-case scenarios:**
    1. custom_rnn_lstm_singleLayer_model_8_22
    2. custom_rnn_lstm_singleLayer_model_9_25
    3. custom_rnn_lstm_singleLayer_model_9_26

  - **Worst-case scenarios:**
    1. custom_rnn_lstm_singleLayer_model_7_19
    2. custom_rnn_lstm_singleLayer_model_8_24
    3. custom_rnn_lstm_multiLayer_model_11_33



|                          | Model                     | Architecture                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Epochs | Batch Size | Learning Rate | Optimizer | Loss Function            | Training Accuracy | Validation Accuracy | Training Loss | Validation Loss | Regularization Functions                                                                             | Key Changes                                                                                         |
|--------------------------|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|------------|---------------|-----------|--------------------------|-------------------|---------------------|---------------|-----------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|
| **best-case scenarios**  | **custom_rnn_lstm_singleLayer_model_8_22**       | **LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES)), Dropout(0.5), Dense(100, activation='relu'), Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.99 (99%)        |  0.99 (99%)    | 0.000821         | 0.200108     | Dropout<br>Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense and LSTM layers                                   |
|                          | **custom_rnn_lstm_singleLayer_model_9_25**       | **LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dropout(0.5), BatchNormalization(), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.0.99 (99%)        |  0.99 (99%)    | 0.0092         | 0.00852    | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense layer, Adding Batch Normalization and Changing the time window          |
|                          | **custom_rnn_lstm_singleLayer_model_9_26**       |LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dropout(0.5), BatchNormalization(), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.0.99 (99%)        |  0.99 (99%)    | 0.000462         | 0.000681     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Changing units on the Dense layer, Adding Batch Normalization and Changing the time window                                       |
| **worst-case scenarios** | **custom_rnn_lstm_singleLayer_model_7_19** | **LSTM(50, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=False), Dense(50, activation='relu'), Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.96 (96%)        | 0.97 (97%)    | 0.087         | 0.09     | Learning Rate Scheduler<br> Early Stopping                         | Initial model with data augmentation                                                                      |
|                          | **custom_rnn_lstm_singleLayer_model_8_24**       |LSTM(100, input_shape=(WINDOW_LENGTH, NUM_FEATURES)), Dropout(0.5), Dense(100, activation='relu'), Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)    | 0.054         | 0.053     | Dropout<br>Learning Rate Scheduler<br> Early Stopping <br> | Changing units on the Dense and LSTM and Changing the time window layers          |
|                          | **custom_rnn_lstm_multiLayer_model_11_33**   | LSTM(units=64, input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=True), Dropout(0.3), LSTM(units=64, return_sequences=False), Dropout(0.3), Dense(units=64, activation='relu'), BatchNormalization(), Dense(units=NUM_CLASSES, activation='softmax') | 50          | 30     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.99 (99%)        | 0.99 (99%)          | 0.0435          | 0.0428            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Adding one more LSTM layer with 64 units, Changing LSTM layer units to 64, Changing dropout values to 0.3 |                                                                                                     |


