### Training Model Comparison

Here are the differences between the models based on their architectures, hyperparameters, performance metrics, and regularization functions applied.

| Model                                 | Architecture                                                                                                                                                                                                                                      | Time Window | Epochs | Batch Size | Learning Rate | Optimizer | Loss Function            | Training Accuracy | Test Accuracy | Training Loss | Test Loss | Regularization Functions                                        | Key Changes                                                                                                  |
|---------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|--------|------------|---------------|-----------|--------------------------|-------------------|---------------|---------------|-----------|-----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| **custom_rnn_simple_singleLayer_model_1_01** | **SimpleRNN(64, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Flatten(), <br> Dense(64, activation='relu'), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.97 (97%)        | 0.97 (97%)    | 0.094         | 0.096     | Learning Rate Scheduler<br> Early Stopping                         | Initial model with data augmentation                                                                       |
| custom_rnn_simple_singleLayer_model_1_02       | SimpleRNN(64, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Flatten(), <br> Dense(64, activation='relu'), <br> Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.92 (92%)        | 0.92 (92%)    | 0.240         | 0.242     | Learning Rate Scheduler<br> Early Stopping                         | Added additional Conv2D and BatchNormalization layers, and Dense layer with 256 units                    |
| custom_rnn_simple_singleLayer_model_1_03       | SimpleRNN(64, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Flatten(), <br> Dense(64, activation='relu'), <br> Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 64         | 0.0002        | Adam      | Categorical Crossentropy | 0.81 (81%)        | 0.81 (81%)    | 0.457         | 0.456     | Learning Rate Scheduler<br> Early Stopping                         | Increased batch size and added Dropout layer after Dense layer                                           |
| **custom_rnn_simple_singleLayer_model_2_04**       | **SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(128, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(3, activation='softmax')** | 20          | 50     | 64         | 0.0002        | Adam      | Categorical Crossentropy | 0.90 (90%)        | 0.90 (90%)    | 0.312         | 0.313     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Added additional Conv2D and BatchNormalization layers, and Dense layer with 512 units                    |
| custom_rnn_simple_singleLayer_model_2_05       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(128, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(3, activation='softmax')       | 30          | 50     | 64         | 0.0002        | Adam      | Categorical Crossentropy | 0.84 (84%)        | 0.84 (84%)    | 0.407         | 0.409     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased learning rate and added L2 regularization to the Dense layer                                   |
| custom_rnn_simple_singleLayer_model_2_06       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(128, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(3, activation='softmax')       | 50          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.83 (83%)        | 0.83 (83%)    | 0.394         | 0.393     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping <br> | Reduced batch size and increased L2 regularization strength                                              |
| **custom_rnn_simple_singleLayer_model_3_07**       | **SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(64, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.71 (71%)        | 0.71 (71%)    | 0.585         | 0.584     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input image size, added Conv2D and BatchNormalization layers, increased Dense layer size         |
| custom_rnn_simple_singleLayer_model_3_08       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(64, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.67 (67%)        | 0.67 (67%)    | 0.669         | 0.671     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Decreased input size, added padding='same' to Conv2D layers, increased number of epochs                  |
| custom_rnn_simple_singleLayer_model_3_09       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(64, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.43 (43%)        | 0.43 (43%)    | 1.07         | 1.07     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Decreased input size, added padding='same' to Conv2D layers, increased number of epochs
| **custom_rnn_simple_multiLayer_model_4_10**       | **SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Dense(10, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.96 (96%)        | 0.96 (96%)          | 0.154          | 0.157            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Further adjusted input size, added padding='same' to Conv2D layers, retained GlobalAveragePooling2D |
| custom_rnn_simple_multiLayer_model_4_11    | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Dense(10, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax') | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.95 (95%)        | 0.95 (95%)          | 0.188          | 0.189            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Added strides to some Conv2D layers, retained GlobalAveragePooling2D, and padding='same' in MaxPooling2D layers |
| custom_rnn_simple_multiLayer_model_4_12   | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Dense(10, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax') | 50          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.41 (41%)        | 0.41 (41%)          | 1.08          | 1.08            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50
| **custom_rnn_simple_multiLayer_model_5_13**   | **SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Flatten(), <br> Dense(100, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.93 (93%)        | 0.93 (93%)          | 0.238          | 0.238            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50      
| custom_rnn_simple_multiLayer_model_5_14   | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Flatten(), <br> Dense(100, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax') | 30          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.56 (56%)        | 0.56 (56%)          | 0.904          | 0.905            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50      
| custom_rnn_simple_multiLayer_model_5_15   | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Flatten(), <br> Dense(100, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax') | 50          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.65 (65%)        | 0.65 (65%)          | 0.686          | 0.684            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50      
| **custom_rnn_simple_multiLayer_model_6_16**   | **SimpleRNN(100, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.2), <br> BatchNormalization(), <br> SimpleRNN(100, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> BatchNormalization(), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)          | 0.078          |0.082            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50      
| custom_rnn_simple_multiLayer_model_6_17   | SimpleRNN(100, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.2), <br> BatchNormalization(), <br> SimpleRNN(100, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> BatchNormalization(), <br> Dense(NUM_CLASSES, activation='softmax') | 30          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.96 (96%)        | 0.96 (96%)          | 0.136          | 0.136            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50      
| custom_rnn_simple_multiLayer_model_6_18   | SimpleRNN(100, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.2), <br> BatchNormalization(), <br> SimpleRNN(100, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> BatchNormalization(), <br> Dense(NUM_CLASSES, activation='softmax') | 50          | 50     | 32         | 0.001        | Adam      | Categorical Crossentropy | 0.47 (47%)        | 0.47 (47%)          | 1.02          | 1.02            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50                                                       |

## Analysis

1. **Architecture**:
    - The architecture changes in order to be improved and each one is trained with 3 different Time Window values.

2. **Hyperparameters**:
    - The epochs, batch size, optimizer, and loss function remain constant across iterations to isolate the effect of architecture changes on performance.

3. **Regularization**:
    - Dropout layers are added to help prevent overfitting.

4. **Performance**:
    - The performance of the models varies according to the variation in time window values ​​as well as the architecture, with no progressive improvement.

This table provides a comprehensive summary of the key changes and performance metrics for each model iteration.

## Analysis

The best and worst case scenarios are:
  - **Best-case scenarios:**
    1. custom_rnn_simple_multiLayer_model_6_16
    2. custom_rnn_simple_multiLayer_model_4_10
    3. custom_rnn_simple_multiLayer_model_4_11

  - **Worst-case scenarios:**
    1. custom_rnn_simple_singleLayer_model_3_09
    2. custom_rnn_simple_multiLayer_model_4_12
    3. custom_rnn_simple_singleLayer_model_3_08



|                          | Model                     | Architecture                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Epochs | Batch Size | Learning Rate | Optimizer | Loss Function            | Training Accuracy | Validation Accuracy | Training Loss | Validation Loss | Regularization Functions                                                                             | Key Changes                                                                                         |
|--------------------------|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|------------|---------------|-----------|--------------------------|-------------------|---------------------|---------------|-----------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|
| **best-case scenarios**  | **custom_rnn_simple_multiLayer_model_6_16**   | SimpleRNN(100, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.2), <br> BatchNormalization(), <br> SimpleRNN(100, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> BatchNormalization(), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.98 (98%)        | 0.98 (98%)          | 0.078          |0.082            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50 |
|                          | **custom_rnn_simple_multiLayer_model_4_10**       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Dense(10, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')** | 20          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.96 (96%)        | 0.96 (96%)          | 0.154          | 0.157            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Further adjusted input size, added padding='same' to Conv2D layers, retained GlobalAveragePooling2D          |
|                          | **custom_rnn_simple_multiLayer_model_4_11**    | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Dense(10, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax') | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.95 (95%)        | 0.95 (95%)          | 0.188          | 0.189            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Added strides to some Conv2D layers, retained GlobalAveragePooling2D, and padding='same' in MaxPooling2D layers                                     |
| **worst-case scenarios** | **custom_rnn_simple_singleLayer_model_3_09**       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(64, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')       | 50          | 50     | 32         | 0.0002        | Adam      | Categorical Crossentropy | 0.43 (43%)        | 0.43 (43%)    | 1.07         | 1.07     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Decreased input size, added padding='same' to Conv2D layers, increased number of epochs          |
|                          | **custom_rnn_simple_multiLayer_model_4_12**   | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES), return_sequences=True), <br> Dropout(0.5), <br> SimpleRNN(64, activation='tanh', return_sequences=False), <br> Dropout(0.5), <br> Dense(10, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax') | 50          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.41 (41%)        | 0.41 (41%)          | 1.08          | 1.08            | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Increased input size to 224x224, extended training epochs to 50          |
|                          | **custom_rnn_simple_singleLayer_model_3_08**       | SimpleRNN(128, activation='tanh', input_shape=(WINDOW_LENGTH, NUM_FEATURES)), <br> Dropout(0.5), <br> Flatten(), <br> Dense(64, activation='relu'), <br> BatchNormalization(), <br> Dropout(0.5), <br> Dense(NUM_CLASSES, activation='softmax')       | 30          | 50     | 32         | 0.0001        | Adam      | Categorical Crossentropy | 0.67 (67%)        | 0.67 (67%)    | 0.669         | 0.671     | Dropout<br>Batch Normalization <br> Learning Rate Scheduler<br> Early Stopping | Decreased input size, added padding='same' to Conv2D layers, increased number of epochs |                                                                                                     |


